{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, 'examples/pytorch/vit/ViT-quantization/ViT-pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modeling import VisionTransformer, CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CONFIGS['ViT-B_16']\n",
    "model = VisionTransformer(config, 384, zero_head=False, num_classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.embeddings.position_embeddings\n",
      "transformer.embeddings.cls_token\n",
      "transformer.embeddings.patch_embeddings.weight\n",
      "transformer.embeddings.patch_embeddings.bias\n",
      "transformer.encoder.layer.0.attention_norm.weight\n",
      "transformer.encoder.layer.0.attention_norm.bias\n",
      "transformer.encoder.layer.0.ffn_norm.weight\n",
      "transformer.encoder.layer.0.ffn_norm.bias\n",
      "transformer.encoder.layer.0.ffn.fc1.weight\n",
      "transformer.encoder.layer.0.ffn.fc1.bias\n",
      "transformer.encoder.layer.0.ffn.fc2.weight\n",
      "transformer.encoder.layer.0.ffn.fc2.bias\n",
      "transformer.encoder.layer.0.attn.query.weight\n",
      "transformer.encoder.layer.0.attn.query.bias\n",
      "transformer.encoder.layer.0.attn.key.weight\n",
      "transformer.encoder.layer.0.attn.key.bias\n",
      "transformer.encoder.layer.0.attn.value.weight\n",
      "transformer.encoder.layer.0.attn.value.bias\n",
      "transformer.encoder.layer.0.attn.out.weight\n",
      "transformer.encoder.layer.0.attn.out.bias\n",
      "transformer.encoder.layer.1.attention_norm.weight\n",
      "transformer.encoder.layer.1.attention_norm.bias\n",
      "transformer.encoder.layer.1.ffn_norm.weight\n",
      "transformer.encoder.layer.1.ffn_norm.bias\n",
      "transformer.encoder.layer.1.ffn.fc1.weight\n",
      "transformer.encoder.layer.1.ffn.fc1.bias\n",
      "transformer.encoder.layer.1.ffn.fc2.weight\n",
      "transformer.encoder.layer.1.ffn.fc2.bias\n",
      "transformer.encoder.layer.1.attn.query.weight\n",
      "transformer.encoder.layer.1.attn.query.bias\n",
      "transformer.encoder.layer.1.attn.key.weight\n",
      "transformer.encoder.layer.1.attn.key.bias\n",
      "transformer.encoder.layer.1.attn.value.weight\n",
      "transformer.encoder.layer.1.attn.value.bias\n",
      "transformer.encoder.layer.1.attn.out.weight\n",
      "transformer.encoder.layer.1.attn.out.bias\n",
      "transformer.encoder.layer.2.attention_norm.weight\n",
      "transformer.encoder.layer.2.attention_norm.bias\n",
      "transformer.encoder.layer.2.ffn_norm.weight\n",
      "transformer.encoder.layer.2.ffn_norm.bias\n",
      "transformer.encoder.layer.2.ffn.fc1.weight\n",
      "transformer.encoder.layer.2.ffn.fc1.bias\n",
      "transformer.encoder.layer.2.ffn.fc2.weight\n",
      "transformer.encoder.layer.2.ffn.fc2.bias\n",
      "transformer.encoder.layer.2.attn.query.weight\n",
      "transformer.encoder.layer.2.attn.query.bias\n",
      "transformer.encoder.layer.2.attn.key.weight\n",
      "transformer.encoder.layer.2.attn.key.bias\n",
      "transformer.encoder.layer.2.attn.value.weight\n",
      "transformer.encoder.layer.2.attn.value.bias\n",
      "transformer.encoder.layer.2.attn.out.weight\n",
      "transformer.encoder.layer.2.attn.out.bias\n",
      "transformer.encoder.layer.3.attention_norm.weight\n",
      "transformer.encoder.layer.3.attention_norm.bias\n",
      "transformer.encoder.layer.3.ffn_norm.weight\n",
      "transformer.encoder.layer.3.ffn_norm.bias\n",
      "transformer.encoder.layer.3.ffn.fc1.weight\n",
      "transformer.encoder.layer.3.ffn.fc1.bias\n",
      "transformer.encoder.layer.3.ffn.fc2.weight\n",
      "transformer.encoder.layer.3.ffn.fc2.bias\n",
      "transformer.encoder.layer.3.attn.query.weight\n",
      "transformer.encoder.layer.3.attn.query.bias\n",
      "transformer.encoder.layer.3.attn.key.weight\n",
      "transformer.encoder.layer.3.attn.key.bias\n",
      "transformer.encoder.layer.3.attn.value.weight\n",
      "transformer.encoder.layer.3.attn.value.bias\n",
      "transformer.encoder.layer.3.attn.out.weight\n",
      "transformer.encoder.layer.3.attn.out.bias\n",
      "transformer.encoder.layer.4.attention_norm.weight\n",
      "transformer.encoder.layer.4.attention_norm.bias\n",
      "transformer.encoder.layer.4.ffn_norm.weight\n",
      "transformer.encoder.layer.4.ffn_norm.bias\n",
      "transformer.encoder.layer.4.ffn.fc1.weight\n",
      "transformer.encoder.layer.4.ffn.fc1.bias\n",
      "transformer.encoder.layer.4.ffn.fc2.weight\n",
      "transformer.encoder.layer.4.ffn.fc2.bias\n",
      "transformer.encoder.layer.4.attn.query.weight\n",
      "transformer.encoder.layer.4.attn.query.bias\n",
      "transformer.encoder.layer.4.attn.key.weight\n",
      "transformer.encoder.layer.4.attn.key.bias\n",
      "transformer.encoder.layer.4.attn.value.weight\n",
      "transformer.encoder.layer.4.attn.value.bias\n",
      "transformer.encoder.layer.4.attn.out.weight\n",
      "transformer.encoder.layer.4.attn.out.bias\n",
      "transformer.encoder.layer.5.attention_norm.weight\n",
      "transformer.encoder.layer.5.attention_norm.bias\n",
      "transformer.encoder.layer.5.ffn_norm.weight\n",
      "transformer.encoder.layer.5.ffn_norm.bias\n",
      "transformer.encoder.layer.5.ffn.fc1.weight\n",
      "transformer.encoder.layer.5.ffn.fc1.bias\n",
      "transformer.encoder.layer.5.ffn.fc2.weight\n",
      "transformer.encoder.layer.5.ffn.fc2.bias\n",
      "transformer.encoder.layer.5.attn.query.weight\n",
      "transformer.encoder.layer.5.attn.query.bias\n",
      "transformer.encoder.layer.5.attn.key.weight\n",
      "transformer.encoder.layer.5.attn.key.bias\n",
      "transformer.encoder.layer.5.attn.value.weight\n",
      "transformer.encoder.layer.5.attn.value.bias\n",
      "transformer.encoder.layer.5.attn.out.weight\n",
      "transformer.encoder.layer.5.attn.out.bias\n",
      "transformer.encoder.layer.6.attention_norm.weight\n",
      "transformer.encoder.layer.6.attention_norm.bias\n",
      "transformer.encoder.layer.6.ffn_norm.weight\n",
      "transformer.encoder.layer.6.ffn_norm.bias\n",
      "transformer.encoder.layer.6.ffn.fc1.weight\n",
      "transformer.encoder.layer.6.ffn.fc1.bias\n",
      "transformer.encoder.layer.6.ffn.fc2.weight\n",
      "transformer.encoder.layer.6.ffn.fc2.bias\n",
      "transformer.encoder.layer.6.attn.query.weight\n",
      "transformer.encoder.layer.6.attn.query.bias\n",
      "transformer.encoder.layer.6.attn.key.weight\n",
      "transformer.encoder.layer.6.attn.key.bias\n",
      "transformer.encoder.layer.6.attn.value.weight\n",
      "transformer.encoder.layer.6.attn.value.bias\n",
      "transformer.encoder.layer.6.attn.out.weight\n",
      "transformer.encoder.layer.6.attn.out.bias\n",
      "transformer.encoder.layer.7.attention_norm.weight\n",
      "transformer.encoder.layer.7.attention_norm.bias\n",
      "transformer.encoder.layer.7.ffn_norm.weight\n",
      "transformer.encoder.layer.7.ffn_norm.bias\n",
      "transformer.encoder.layer.7.ffn.fc1.weight\n",
      "transformer.encoder.layer.7.ffn.fc1.bias\n",
      "transformer.encoder.layer.7.ffn.fc2.weight\n",
      "transformer.encoder.layer.7.ffn.fc2.bias\n",
      "transformer.encoder.layer.7.attn.query.weight\n",
      "transformer.encoder.layer.7.attn.query.bias\n",
      "transformer.encoder.layer.7.attn.key.weight\n",
      "transformer.encoder.layer.7.attn.key.bias\n",
      "transformer.encoder.layer.7.attn.value.weight\n",
      "transformer.encoder.layer.7.attn.value.bias\n",
      "transformer.encoder.layer.7.attn.out.weight\n",
      "transformer.encoder.layer.7.attn.out.bias\n",
      "transformer.encoder.layer.8.attention_norm.weight\n",
      "transformer.encoder.layer.8.attention_norm.bias\n",
      "transformer.encoder.layer.8.ffn_norm.weight\n",
      "transformer.encoder.layer.8.ffn_norm.bias\n",
      "transformer.encoder.layer.8.ffn.fc1.weight\n",
      "transformer.encoder.layer.8.ffn.fc1.bias\n",
      "transformer.encoder.layer.8.ffn.fc2.weight\n",
      "transformer.encoder.layer.8.ffn.fc2.bias\n",
      "transformer.encoder.layer.8.attn.query.weight\n",
      "transformer.encoder.layer.8.attn.query.bias\n",
      "transformer.encoder.layer.8.attn.key.weight\n",
      "transformer.encoder.layer.8.attn.key.bias\n",
      "transformer.encoder.layer.8.attn.value.weight\n",
      "transformer.encoder.layer.8.attn.value.bias\n",
      "transformer.encoder.layer.8.attn.out.weight\n",
      "transformer.encoder.layer.8.attn.out.bias\n",
      "transformer.encoder.layer.9.attention_norm.weight\n",
      "transformer.encoder.layer.9.attention_norm.bias\n",
      "transformer.encoder.layer.9.ffn_norm.weight\n",
      "transformer.encoder.layer.9.ffn_norm.bias\n",
      "transformer.encoder.layer.9.ffn.fc1.weight\n",
      "transformer.encoder.layer.9.ffn.fc1.bias\n",
      "transformer.encoder.layer.9.ffn.fc2.weight\n",
      "transformer.encoder.layer.9.ffn.fc2.bias\n",
      "transformer.encoder.layer.9.attn.query.weight\n",
      "transformer.encoder.layer.9.attn.query.bias\n",
      "transformer.encoder.layer.9.attn.key.weight\n",
      "transformer.encoder.layer.9.attn.key.bias\n",
      "transformer.encoder.layer.9.attn.value.weight\n",
      "transformer.encoder.layer.9.attn.value.bias\n",
      "transformer.encoder.layer.9.attn.out.weight\n",
      "transformer.encoder.layer.9.attn.out.bias\n",
      "transformer.encoder.layer.10.attention_norm.weight\n",
      "transformer.encoder.layer.10.attention_norm.bias\n",
      "transformer.encoder.layer.10.ffn_norm.weight\n",
      "transformer.encoder.layer.10.ffn_norm.bias\n",
      "transformer.encoder.layer.10.ffn.fc1.weight\n",
      "transformer.encoder.layer.10.ffn.fc1.bias\n",
      "transformer.encoder.layer.10.ffn.fc2.weight\n",
      "transformer.encoder.layer.10.ffn.fc2.bias\n",
      "transformer.encoder.layer.10.attn.query.weight\n",
      "transformer.encoder.layer.10.attn.query.bias\n",
      "transformer.encoder.layer.10.attn.key.weight\n",
      "transformer.encoder.layer.10.attn.key.bias\n",
      "transformer.encoder.layer.10.attn.value.weight\n",
      "transformer.encoder.layer.10.attn.value.bias\n",
      "transformer.encoder.layer.10.attn.out.weight\n",
      "transformer.encoder.layer.10.attn.out.bias\n",
      "transformer.encoder.layer.11.attention_norm.weight\n",
      "transformer.encoder.layer.11.attention_norm.bias\n",
      "transformer.encoder.layer.11.ffn_norm.weight\n",
      "transformer.encoder.layer.11.ffn_norm.bias\n",
      "transformer.encoder.layer.11.ffn.fc1.weight\n",
      "transformer.encoder.layer.11.ffn.fc1.bias\n",
      "transformer.encoder.layer.11.ffn.fc2.weight\n",
      "transformer.encoder.layer.11.ffn.fc2.bias\n",
      "transformer.encoder.layer.11.attn.query.weight\n",
      "transformer.encoder.layer.11.attn.query.bias\n",
      "transformer.encoder.layer.11.attn.key.weight\n",
      "transformer.encoder.layer.11.attn.key.bias\n",
      "transformer.encoder.layer.11.attn.value.weight\n",
      "transformer.encoder.layer.11.attn.value.bias\n",
      "transformer.encoder.layer.11.attn.out.weight\n",
      "transformer.encoder.layer.11.attn.out.bias\n",
      "transformer.encoder.encoder_norm.weight\n",
      "transformer.encoder.encoder_norm.bias\n",
      "head.weight\n",
      "head.bias\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_from(np.load(args.pretrained_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (embeddings): Embeddings(\n",
       "      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_npz = np.load('ViT-B_16.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transformer/encoder_norm/bias',\n",
       " 'Transformer/encoder_norm/scale',\n",
       " 'Transformer/encoderblock_0/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_0/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_0/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_0/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_0/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_0/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_0/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_0/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_1/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_1/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_1/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_1/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_1/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_1/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_1/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_1/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_10/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_10/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_10/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_10/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_10/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_10/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_10/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_10/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_11/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_11/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_11/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_11/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_11/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_11/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_11/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_11/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_2/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_2/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_2/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_2/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_2/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_2/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_2/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_2/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_3/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_3/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_3/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_3/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_3/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_3/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_3/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_3/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_4/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_4/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_4/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_4/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_4/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_4/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_4/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_4/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_5/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_5/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_5/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_5/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_5/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_5/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_5/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_5/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_6/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_6/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_6/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_6/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_6/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_6/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_6/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_6/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_7/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_7/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_7/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_7/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_7/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_7/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_7/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_7/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_8/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_8/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_8/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_8/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_8/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_8/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_8/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_8/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_9/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_9/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_9/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_9/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_9/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_9/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_9/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_9/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/posembed_input/pos_embedding',\n",
       " 'cls',\n",
       " 'embedding/bias',\n",
       " 'embedding/kernel',\n",
       " 'head/bias',\n",
       " 'head/kernel']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head.bias head.bias\n",
      "head.weight head.weight\n",
      "transformer.embeddings.cls_token transformer.embeddings.cls_token\n",
      "transformer.embeddings.patch_embeddings.bias transformer.embeddings.patch_embeddings.bias\n",
      "transformer.embeddings.patch_embeddings.weight transformer.embeddings.patch_embeddings.weight\n",
      "transformer.embeddings.position_embeddings transformer.embeddings.position_embeddings\n",
      "transformer.encoder.encoder_norm.bias transformer.encoder.encoder_norm.bias\n",
      "transformer.encoder.encoder_norm.weight transformer.encoder.encoder_norm.weight\n",
      "transformer.encoder.layer.0.attention_norm.bias transformer.encoder.layer.0.MlpBlock.3.Dense.0.bias\n",
      "transformer.encoder.layer.0.attention_norm.weight transformer.encoder.layer.0.MlpBlock.3.Dense.0.weight\n",
      "transformer.encoder.layer.0.attn.key.bias transformer.encoder.layer.0.MlpBlock.3.Dense.1.bias\n",
      "transformer.encoder.layer.0.attn.key.weight transformer.encoder.layer.0.MlpBlock.3.Dense.1.weight\n",
      "transformer.encoder.layer.0.attn.out.bias transformer.encoder.layer.0.attention_norm.bias\n",
      "transformer.encoder.layer.0.attn.out.weight transformer.encoder.layer.0.attention_norm.bias\n",
      "transformer.encoder.layer.0.attn.query.bias transformer.encoder.layer.0.attention_norm.weight\n",
      "transformer.encoder.layer.0.attn.query.weight transformer.encoder.layer.0.attention_norm.weight\n",
      "transformer.encoder.layer.0.attn.value.bias transformer.encoder.layer.0.attn.1.key.bias\n",
      "transformer.encoder.layer.0.attn.value.weight transformer.encoder.layer.0.attn.1.key.weight\n",
      "transformer.encoder.layer.0.ffn.fc1.bias transformer.encoder.layer.0.attn.1.out.bias\n",
      "transformer.encoder.layer.0.ffn.fc1.weight transformer.encoder.layer.0.attn.1.out.weight\n",
      "transformer.encoder.layer.0.ffn.fc2.bias transformer.encoder.layer.0.attn.1.query.bias\n",
      "transformer.encoder.layer.0.ffn.fc2.weight transformer.encoder.layer.0.attn.1.query.weight\n",
      "transformer.encoder.layer.0.ffn_norm.bias transformer.encoder.layer.0.attn.1.value.bias\n",
      "transformer.encoder.layer.0.ffn_norm.weight transformer.encoder.layer.0.attn.1.value.weight\n",
      "transformer.encoder.layer.1.attention_norm.bias transformer.encoder.layer.1.MlpBlock.3.Dense.0.bias\n",
      "transformer.encoder.layer.1.attention_norm.weight transformer.encoder.layer.1.MlpBlock.3.Dense.0.weight\n",
      "transformer.encoder.layer.1.attn.key.bias transformer.encoder.layer.1.MlpBlock.3.Dense.1.bias\n",
      "transformer.encoder.layer.1.attn.key.weight transformer.encoder.layer.1.MlpBlock.3.Dense.1.weight\n",
      "transformer.encoder.layer.1.attn.out.bias transformer.encoder.layer.1.attention_norm.bias\n",
      "transformer.encoder.layer.1.attn.out.weight transformer.encoder.layer.1.attention_norm.bias\n",
      "transformer.encoder.layer.1.attn.query.bias transformer.encoder.layer.1.attention_norm.weight\n",
      "transformer.encoder.layer.1.attn.query.weight transformer.encoder.layer.1.attention_norm.weight\n",
      "transformer.encoder.layer.1.attn.value.bias transformer.encoder.layer.1.attn.1.key.bias\n",
      "transformer.encoder.layer.1.attn.value.weight transformer.encoder.layer.1.attn.1.key.weight\n",
      "transformer.encoder.layer.1.ffn.fc1.bias transformer.encoder.layer.1.attn.1.out.bias\n",
      "transformer.encoder.layer.1.ffn.fc1.weight transformer.encoder.layer.1.attn.1.out.weight\n",
      "transformer.encoder.layer.1.ffn.fc2.bias transformer.encoder.layer.1.attn.1.query.bias\n",
      "transformer.encoder.layer.1.ffn.fc2.weight transformer.encoder.layer.1.attn.1.query.weight\n",
      "transformer.encoder.layer.1.ffn_norm.bias transformer.encoder.layer.1.attn.1.value.bias\n",
      "transformer.encoder.layer.1.ffn_norm.weight transformer.encoder.layer.1.attn.1.value.weight\n",
      "transformer.encoder.layer.10.attention_norm.bias transformer.encoder.layer.10.MlpBlock.3.Dense.0.bias\n",
      "transformer.encoder.layer.10.attention_norm.weight transformer.encoder.layer.10.MlpBlock.3.Dense.0.weight\n",
      "transformer.encoder.layer.10.attn.key.bias transformer.encoder.layer.10.MlpBlock.3.Dense.1.bias\n",
      "transformer.encoder.layer.10.attn.key.weight transformer.encoder.layer.10.MlpBlock.3.Dense.1.weight\n",
      "transformer.encoder.layer.10.attn.out.bias transformer.encoder.layer.10.attention_norm.bias\n",
      "transformer.encoder.layer.10.attn.out.weight transformer.encoder.layer.10.attention_norm.bias\n",
      "transformer.encoder.layer.10.attn.query.bias transformer.encoder.layer.10.attention_norm.weight\n",
      "transformer.encoder.layer.10.attn.query.weight transformer.encoder.layer.10.attention_norm.weight\n",
      "transformer.encoder.layer.10.attn.value.bias transformer.encoder.layer.10.attn.1.key.bias\n",
      "transformer.encoder.layer.10.attn.value.weight transformer.encoder.layer.10.attn.1.key.weight\n",
      "transformer.encoder.layer.10.ffn.fc1.bias transformer.encoder.layer.10.attn.1.out.bias\n",
      "transformer.encoder.layer.10.ffn.fc1.weight transformer.encoder.layer.10.attn.1.out.weight\n",
      "transformer.encoder.layer.10.ffn.fc2.bias transformer.encoder.layer.10.attn.1.query.bias\n",
      "transformer.encoder.layer.10.ffn.fc2.weight transformer.encoder.layer.10.attn.1.query.weight\n",
      "transformer.encoder.layer.10.ffn_norm.bias transformer.encoder.layer.10.attn.1.value.bias\n",
      "transformer.encoder.layer.10.ffn_norm.weight transformer.encoder.layer.10.attn.1.value.weight\n",
      "transformer.encoder.layer.11.attention_norm.bias transformer.encoder.layer.11.MlpBlock.3.Dense.0.bias\n",
      "transformer.encoder.layer.11.attention_norm.weight transformer.encoder.layer.11.MlpBlock.3.Dense.0.weight\n",
      "transformer.encoder.layer.11.attn.key.bias transformer.encoder.layer.11.MlpBlock.3.Dense.1.bias\n",
      "transformer.encoder.layer.11.attn.key.weight transformer.encoder.layer.11.MlpBlock.3.Dense.1.weight\n",
      "transformer.encoder.layer.11.attn.out.bias transformer.encoder.layer.11.attention_norm.bias\n",
      "transformer.encoder.layer.11.attn.out.weight transformer.encoder.layer.11.attention_norm.bias\n",
      "transformer.encoder.layer.11.attn.query.bias transformer.encoder.layer.11.attention_norm.weight\n",
      "transformer.encoder.layer.11.attn.query.weight transformer.encoder.layer.11.attention_norm.weight\n",
      "transformer.encoder.layer.11.attn.value.bias transformer.encoder.layer.11.attn.1.key.bias\n",
      "transformer.encoder.layer.11.attn.value.weight transformer.encoder.layer.11.attn.1.key.weight\n",
      "transformer.encoder.layer.11.ffn.fc1.bias transformer.encoder.layer.11.attn.1.out.bias\n",
      "transformer.encoder.layer.11.ffn.fc1.weight transformer.encoder.layer.11.attn.1.out.weight\n",
      "transformer.encoder.layer.11.ffn.fc2.bias transformer.encoder.layer.11.attn.1.query.bias\n",
      "transformer.encoder.layer.11.ffn.fc2.weight transformer.encoder.layer.11.attn.1.query.weight\n",
      "transformer.encoder.layer.11.ffn_norm.bias transformer.encoder.layer.11.attn.1.value.bias\n",
      "transformer.encoder.layer.11.ffn_norm.weight transformer.encoder.layer.11.attn.1.value.weight\n",
      "transformer.encoder.layer.2.attention_norm.bias transformer.encoder.layer.2.MlpBlock.3.Dense.0.bias\n",
      "transformer.encoder.layer.2.attention_norm.weight transformer.encoder.layer.2.MlpBlock.3.Dense.0.weight\n",
      "transformer.encoder.layer.2.attn.key.bias transformer.encoder.layer.2.MlpBlock.3.Dense.1.bias\n",
      "transformer.encoder.layer.2.attn.key.weight transformer.encoder.layer.2.MlpBlock.3.Dense.1.weight\n",
      "transformer.encoder.layer.2.attn.out.bias transformer.encoder.layer.2.attention_norm.bias\n",
      "transformer.encoder.layer.2.attn.out.weight transformer.encoder.layer.2.attention_norm.bias\n",
      "transformer.encoder.layer.2.attn.query.bias transformer.encoder.layer.2.attention_norm.weight\n",
      "transformer.encoder.layer.2.attn.query.weight transformer.encoder.layer.2.attention_norm.weight\n",
      "transformer.encoder.layer.2.attn.value.bias transformer.encoder.layer.2.attn.1.key.bias\n",
      "transformer.encoder.layer.2.attn.value.weight transformer.encoder.layer.2.attn.1.key.weight\n",
      "transformer.encoder.layer.2.ffn.fc1.bias transformer.encoder.layer.2.attn.1.out.bias\n",
      "transformer.encoder.layer.2.ffn.fc1.weight transformer.encoder.layer.2.attn.1.out.weight\n",
      "transformer.encoder.layer.2.ffn.fc2.bias transformer.encoder.layer.2.attn.1.query.bias\n",
      "transformer.encoder.layer.2.ffn.fc2.weight transformer.encoder.layer.2.attn.1.query.weight\n",
      "transformer.encoder.layer.2.ffn_norm.bias transformer.encoder.layer.2.attn.1.value.bias\n",
      "transformer.encoder.layer.2.ffn_norm.weight transformer.encoder.layer.2.attn.1.value.weight\n",
      "transformer.encoder.layer.3.attention_norm.bias transformer.encoder.layer.3.MlpBlock.3.Dense.0.bias\n",
      "transformer.encoder.layer.3.attention_norm.weight transformer.encoder.layer.3.MlpBlock.3.Dense.0.weight\n",
      "transformer.encoder.layer.3.attn.key.bias transformer.encoder.layer.3.MlpBlock.3.Dense.1.bias\n",
      "transformer.encoder.layer.3.attn.key.weight transformer.encoder.layer.3.MlpBlock.3.Dense.1.weight\n",
      "transformer.encoder.layer.3.attn.out.bias transformer.encoder.layer.3.attention_norm.bias\n",
      "transformer.encoder.layer.3.attn.out.weight transformer.encoder.layer.3.attention_norm.bias\n",
      "transformer.encoder.layer.3.attn.query.bias transformer.encoder.layer.3.attention_norm.weight\n",
      "transformer.encoder.layer.3.attn.query.weight transformer.encoder.layer.3.attention_norm.weight\n",
      "transformer.encoder.layer.3.attn.value.bias transformer.encoder.layer.3.attn.1.key.bias\n",
      "transformer.encoder.layer.3.attn.value.weight transformer.encoder.layer.3.attn.1.key.weight\n",
      "transformer.encoder.layer.3.ffn.fc1.bias transformer.encoder.layer.3.attn.1.out.bias\n",
      "transformer.encoder.layer.3.ffn.fc1.weight transformer.encoder.layer.3.attn.1.out.weight\n",
      "transformer.encoder.layer.3.ffn.fc2.bias transformer.encoder.layer.3.attn.1.query.bias\n",
      "transformer.encoder.layer.3.ffn.fc2.weight transformer.encoder.layer.3.attn.1.query.weight\n",
      "transformer.encoder.layer.3.ffn_norm.bias transformer.encoder.layer.3.attn.1.value.bias\n",
      "transformer.encoder.layer.3.ffn_norm.weight transformer.encoder.layer.3.attn.1.value.weight\n",
      "transformer.encoder.layer.4.attention_norm.bias transformer.encoder.layer.4.MlpBlock.3.Dense.0.bias\n",
      "transformer.encoder.layer.4.attention_norm.weight transformer.encoder.layer.4.MlpBlock.3.Dense.0.weight\n",
      "transformer.encoder.layer.4.attn.key.bias transformer.encoder.layer.4.MlpBlock.3.Dense.1.bias\n",
      "transformer.encoder.layer.4.attn.key.weight transformer.encoder.layer.4.MlpBlock.3.Dense.1.weight\n",
      "transformer.encoder.layer.4.attn.out.bias transformer.encoder.layer.4.attention_norm.bias\n",
      "transformer.encoder.layer.4.attn.out.weight transformer.encoder.layer.4.attention_norm.bias\n",
      "transformer.encoder.layer.4.attn.query.bias transformer.encoder.layer.4.attention_norm.weight\n",
      "transformer.encoder.layer.4.attn.query.weight transformer.encoder.layer.4.attention_norm.weight\n",
      "transformer.encoder.layer.4.attn.value.bias transformer.encoder.layer.4.attn.1.key.bias\n",
      "transformer.encoder.layer.4.attn.value.weight transformer.encoder.layer.4.attn.1.key.weight\n",
      "transformer.encoder.layer.4.ffn.fc1.bias transformer.encoder.layer.4.attn.1.out.bias\n",
      "transformer.encoder.layer.4.ffn.fc1.weight transformer.encoder.layer.4.attn.1.out.weight\n",
      "transformer.encoder.layer.4.ffn.fc2.bias transformer.encoder.layer.4.attn.1.query.bias\n",
      "transformer.encoder.layer.4.ffn.fc2.weight transformer.encoder.layer.4.attn.1.query.weight\n",
      "transformer.encoder.layer.4.ffn_norm.bias transformer.encoder.layer.4.attn.1.value.bias\n",
      "transformer.encoder.layer.4.ffn_norm.weight transformer.encoder.layer.4.attn.1.value.weight\n",
      "transformer.encoder.layer.5.attention_norm.bias transformer.encoder.layer.5.MlpBlock.3.Dense.0.bias\n",
      "transformer.encoder.layer.5.attention_norm.weight transformer.encoder.layer.5.MlpBlock.3.Dense.0.weight\n",
      "transformer.encoder.layer.5.attn.key.bias transformer.encoder.layer.5.MlpBlock.3.Dense.1.bias\n",
      "transformer.encoder.layer.5.attn.key.weight transformer.encoder.layer.5.MlpBlock.3.Dense.1.weight\n",
      "transformer.encoder.layer.5.attn.out.bias transformer.encoder.layer.5.attention_norm.bias\n",
      "transformer.encoder.layer.5.attn.out.weight transformer.encoder.layer.5.attention_norm.bias\n",
      "transformer.encoder.layer.5.attn.query.bias transformer.encoder.layer.5.attention_norm.weight\n",
      "transformer.encoder.layer.5.attn.query.weight transformer.encoder.layer.5.attention_norm.weight\n",
      "transformer.encoder.layer.5.attn.value.bias transformer.encoder.layer.5.attn.1.key.bias\n",
      "transformer.encoder.layer.5.attn.value.weight transformer.encoder.layer.5.attn.1.key.weight\n",
      "transformer.encoder.layer.5.ffn.fc1.bias transformer.encoder.layer.5.attn.1.out.bias\n",
      "transformer.encoder.layer.5.ffn.fc1.weight transformer.encoder.layer.5.attn.1.out.weight\n",
      "transformer.encoder.layer.5.ffn.fc2.bias transformer.encoder.layer.5.attn.1.query.bias\n",
      "transformer.encoder.layer.5.ffn.fc2.weight transformer.encoder.layer.5.attn.1.query.weight\n",
      "transformer.encoder.layer.5.ffn_norm.bias transformer.encoder.layer.5.attn.1.value.bias\n",
      "transformer.encoder.layer.5.ffn_norm.weight transformer.encoder.layer.5.attn.1.value.weight\n",
      "transformer.encoder.layer.6.attention_norm.bias transformer.encoder.layer.6.MlpBlock.3.Dense.0.bias\n",
      "transformer.encoder.layer.6.attention_norm.weight transformer.encoder.layer.6.MlpBlock.3.Dense.0.weight\n",
      "transformer.encoder.layer.6.attn.key.bias transformer.encoder.layer.6.MlpBlock.3.Dense.1.bias\n",
      "transformer.encoder.layer.6.attn.key.weight transformer.encoder.layer.6.MlpBlock.3.Dense.1.weight\n",
      "transformer.encoder.layer.6.attn.out.bias transformer.encoder.layer.6.attention_norm.bias\n",
      "transformer.encoder.layer.6.attn.out.weight transformer.encoder.layer.6.attention_norm.bias\n",
      "transformer.encoder.layer.6.attn.query.bias transformer.encoder.layer.6.attention_norm.weight\n",
      "transformer.encoder.layer.6.attn.query.weight transformer.encoder.layer.6.attention_norm.weight\n",
      "transformer.encoder.layer.6.attn.value.bias transformer.encoder.layer.6.attn.1.key.bias\n",
      "transformer.encoder.layer.6.attn.value.weight transformer.encoder.layer.6.attn.1.key.weight\n",
      "transformer.encoder.layer.6.ffn.fc1.bias transformer.encoder.layer.6.attn.1.out.bias\n",
      "transformer.encoder.layer.6.ffn.fc1.weight transformer.encoder.layer.6.attn.1.out.weight\n",
      "transformer.encoder.layer.6.ffn.fc2.bias transformer.encoder.layer.6.attn.1.query.bias\n",
      "transformer.encoder.layer.6.ffn.fc2.weight transformer.encoder.layer.6.attn.1.query.weight\n",
      "transformer.encoder.layer.6.ffn_norm.bias transformer.encoder.layer.6.attn.1.value.bias\n",
      "transformer.encoder.layer.6.ffn_norm.weight transformer.encoder.layer.6.attn.1.value.weight\n",
      "transformer.encoder.layer.7.attention_norm.bias transformer.encoder.layer.7.MlpBlock.3.Dense.0.bias\n",
      "transformer.encoder.layer.7.attention_norm.weight transformer.encoder.layer.7.MlpBlock.3.Dense.0.weight\n",
      "transformer.encoder.layer.7.attn.key.bias transformer.encoder.layer.7.MlpBlock.3.Dense.1.bias\n",
      "transformer.encoder.layer.7.attn.key.weight transformer.encoder.layer.7.MlpBlock.3.Dense.1.weight\n",
      "transformer.encoder.layer.7.attn.out.bias transformer.encoder.layer.7.attention_norm.bias\n",
      "transformer.encoder.layer.7.attn.out.weight transformer.encoder.layer.7.attention_norm.bias\n",
      "transformer.encoder.layer.7.attn.query.bias transformer.encoder.layer.7.attention_norm.weight\n",
      "transformer.encoder.layer.7.attn.query.weight transformer.encoder.layer.7.attention_norm.weight\n",
      "transformer.encoder.layer.7.attn.value.bias transformer.encoder.layer.7.attn.1.key.bias\n",
      "transformer.encoder.layer.7.attn.value.weight transformer.encoder.layer.7.attn.1.key.weight\n",
      "transformer.encoder.layer.7.ffn.fc1.bias transformer.encoder.layer.7.attn.1.out.bias\n",
      "transformer.encoder.layer.7.ffn.fc1.weight transformer.encoder.layer.7.attn.1.out.weight\n",
      "transformer.encoder.layer.7.ffn.fc2.bias transformer.encoder.layer.7.attn.1.query.bias\n",
      "transformer.encoder.layer.7.ffn.fc2.weight transformer.encoder.layer.7.attn.1.query.weight\n",
      "transformer.encoder.layer.7.ffn_norm.bias transformer.encoder.layer.7.attn.1.value.bias\n",
      "transformer.encoder.layer.7.ffn_norm.weight transformer.encoder.layer.7.attn.1.value.weight\n",
      "transformer.encoder.layer.8.attention_norm.bias transformer.encoder.layer.8.MlpBlock.3.Dense.0.bias\n",
      "transformer.encoder.layer.8.attention_norm.weight transformer.encoder.layer.8.MlpBlock.3.Dense.0.weight\n",
      "transformer.encoder.layer.8.attn.key.bias transformer.encoder.layer.8.MlpBlock.3.Dense.1.bias\n",
      "transformer.encoder.layer.8.attn.key.weight transformer.encoder.layer.8.MlpBlock.3.Dense.1.weight\n",
      "transformer.encoder.layer.8.attn.out.bias transformer.encoder.layer.8.attention_norm.bias\n",
      "transformer.encoder.layer.8.attn.out.weight transformer.encoder.layer.8.attention_norm.bias\n",
      "transformer.encoder.layer.8.attn.query.bias transformer.encoder.layer.8.attention_norm.weight\n",
      "transformer.encoder.layer.8.attn.query.weight transformer.encoder.layer.8.attention_norm.weight\n",
      "transformer.encoder.layer.8.attn.value.bias transformer.encoder.layer.8.attn.1.key.bias\n",
      "transformer.encoder.layer.8.attn.value.weight transformer.encoder.layer.8.attn.1.key.weight\n",
      "transformer.encoder.layer.8.ffn.fc1.bias transformer.encoder.layer.8.attn.1.out.bias\n",
      "transformer.encoder.layer.8.ffn.fc1.weight transformer.encoder.layer.8.attn.1.out.weight\n",
      "transformer.encoder.layer.8.ffn.fc2.bias transformer.encoder.layer.8.attn.1.query.bias\n",
      "transformer.encoder.layer.8.ffn.fc2.weight transformer.encoder.layer.8.attn.1.query.weight\n",
      "transformer.encoder.layer.8.ffn_norm.bias transformer.encoder.layer.8.attn.1.value.bias\n",
      "transformer.encoder.layer.8.ffn_norm.weight transformer.encoder.layer.8.attn.1.value.weight\n",
      "transformer.encoder.layer.9.attention_norm.bias transformer.encoder.layer.9.MlpBlock.3.Dense.0.bias\n",
      "transformer.encoder.layer.9.attention_norm.weight transformer.encoder.layer.9.MlpBlock.3.Dense.0.weight\n",
      "transformer.encoder.layer.9.attn.key.bias transformer.encoder.layer.9.MlpBlock.3.Dense.1.bias\n",
      "transformer.encoder.layer.9.attn.key.weight transformer.encoder.layer.9.MlpBlock.3.Dense.1.weight\n",
      "transformer.encoder.layer.9.attn.out.bias transformer.encoder.layer.9.attention_norm.bias\n",
      "transformer.encoder.layer.9.attn.out.weight transformer.encoder.layer.9.attention_norm.bias\n",
      "transformer.encoder.layer.9.attn.query.bias transformer.encoder.layer.9.attention_norm.weight\n",
      "transformer.encoder.layer.9.attn.query.weight transformer.encoder.layer.9.attention_norm.weight\n",
      "transformer.encoder.layer.9.attn.value.bias transformer.encoder.layer.9.attn.1.key.bias\n",
      "transformer.encoder.layer.9.attn.value.weight transformer.encoder.layer.9.attn.1.key.weight\n",
      "transformer.encoder.layer.9.ffn.fc1.bias transformer.encoder.layer.9.attn.1.out.bias\n",
      "transformer.encoder.layer.9.ffn.fc1.weight transformer.encoder.layer.9.attn.1.out.weight\n",
      "transformer.encoder.layer.9.ffn.fc2.bias transformer.encoder.layer.9.attn.1.query.bias\n",
      "transformer.encoder.layer.9.ffn.fc2.weight transformer.encoder.layer.9.attn.1.query.weight\n",
      "transformer.encoder.layer.9.ffn_norm.bias transformer.encoder.layer.9.attn.1.value.bias\n",
      "transformer.encoder.layer.9.ffn_norm.weight transformer.encoder.layer.9.attn.1.value.weight\n"
     ]
    }
   ],
   "source": [
    "l1, l2 = [k for k in model.state_dict()], list(pretrained_npz.keys())\n",
    "\n",
    "l1.sort()\n",
    "l2.sort()\n",
    "\n",
    "i = 0\n",
    "\n",
    "new_l2 = []\n",
    "for k1, k2 in zip(l1, l2):\n",
    "    k2 = k2.replace('Transformer', 'transformer')\n",
    "\n",
    "    items = k2.split('/')\n",
    "    items = [item.replace('_', '.') for item in items]\n",
    "    items = ('.'.join(items)).split('.')\n",
    "\n",
    "    for i, item in enumerate(items):\n",
    "        items[i] = item.replace('MultiHeadDotProductAttention', 'attn')\n",
    "        items[i] = items[i].replace('kernel', 'weight')\n",
    "        items[i] = items[i].replace('scale', 'weight')\n",
    "        items[i] = items[i].replace('encoderblock', 'encoder.layer')\n",
    "        \n",
    "    k2 = '.'.join(items)\n",
    "    new_l2.append(k2)\n",
    "    i += 1\n",
    "\n",
    "\n",
    "l2 = new_l2\n",
    "new_l2 = []\n",
    "for i, k in enumerate(l2):\n",
    "    if k == 'cls':\n",
    "        l2[i] = 'transformer.embeddings.cls_token'\n",
    "    elif k == 'embedding.bias':\n",
    "        l2[i] = 'transformer.embeddings.patch_embeddings.bias'\n",
    "    elif k == 'embedding.weight':\n",
    "        l2[i] = 'transformer.embeddings.patch_embeddings.weight'\n",
    "    elif k == 'transformer.encoder.norm.bias':\n",
    "        l2[i] = 'transformer.encoder.encoder_norm.bias'\n",
    "    elif k == 'transformer.encoder.norm.weight':\n",
    "        l2[i] = 'transformer.encoder.encoder_norm.weight'\n",
    "    elif k == 'transformer.posembed.input.pos.embedding':\n",
    "        l2[i] = 'transformer.embeddings.position_embeddings' \n",
    "    else:\n",
    "\n",
    "        l2[i] = l2[i].replace('LayerNorm.0', 'attention_norm')\n",
    "        l2[i] = l2[i].replace('LayerNorm.1', 'attention_norm')\n",
    "        l2[i] = l2[i].replace('LayerNorm.2', 'attention_norm')\n",
    "        # print(l2[i])\n",
    "    # new_l2.append(l2[i])\n",
    "\n",
    "# l2 = new_l2\n",
    "l1.sort()\n",
    "l2.sort()\n",
    "\n",
    "for k1, k2 in zip(l1, l2):\n",
    "    print(k1, k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e566f0cef0e58438052edc5c90776e243c90880e682bc75fa4b0c6e5f795fc07"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('trtpy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
